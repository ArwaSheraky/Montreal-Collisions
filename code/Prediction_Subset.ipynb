{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master('spark://master:7077').appName(\"Jupyter\").getOrCreate()\n",
    "spark = SparkSession.builder.master('local[1]').appName(\"Jupyter\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# sc = SparkContext('local') \n",
    "# spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/accidents_new.csv', header='true', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'DATE',\n",
       " 'WEEK_DAY',\n",
       " 'REG',\n",
       " 'MUNCP',\n",
       " 'STREET',\n",
       " 'TYPE_ACCDN',\n",
       " 'SURFACE',\n",
       " 'LIGHT',\n",
       " 'STR_ASPCT',\n",
       " 'STR_CONFIG',\n",
       " 'METEO',\n",
       " 'GRAVITE',\n",
       " 'NB_VEH_IMPLIQUES_ACCDN',\n",
       " 'NB_VICTIMES_TOTAL',\n",
       " 'NB_MORTS',\n",
       " 'NB_BLESSES_GRAVES',\n",
       " 'NB_BLESS_LEGERS',\n",
       " 'NB_DECES_PIETON',\n",
       " 'NB_BLESSES_PIETON',\n",
       " 'NB_VICTIMES_PIETON',\n",
       " 'NB_DECES_MOTO',\n",
       " 'NB_BLESSES_MOTO',\n",
       " 'NB_VICTIMES_MOTO',\n",
       " 'NB_DECES_VELO',\n",
       " 'NB_BLESSES_VELO',\n",
       " 'NB_VICTIMES_VELO',\n",
       " 'nb_automobile_camion_leger',\n",
       " 'nb_camionLourd_tractRoutier',\n",
       " 'nb_outil_equipement',\n",
       " 'nb_tous_autobus_minibus',\n",
       " 'nb_bicyclette',\n",
       " 'nb_cyclomoteur',\n",
       " 'nb_motocyclette',\n",
       " 'nb_taxi',\n",
       " 'nb_urgence',\n",
       " 'nb_motoneige',\n",
       " 'nb_VHR',\n",
       " 'nb_autres_types',\n",
       " 'nb_veh_non_precise']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- WEEK_DAY: string (nullable = true)\n",
      " |-- REG: string (nullable = true)\n",
      " |-- MUNCP: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- TYPE_ACCDN: integer (nullable = true)\n",
      " |-- SURFACE: integer (nullable = true)\n",
      " |-- LIGHT: integer (nullable = true)\n",
      " |-- STR_ASPCT: integer (nullable = true)\n",
      " |-- STR_CONFIG: integer (nullable = true)\n",
      " |-- METEO: integer (nullable = true)\n",
      " |-- GRAVITE: string (nullable = true)\n",
      " |-- NB_VEH_IMPLIQUES_ACCDN: integer (nullable = true)\n",
      " |-- NB_VICTIMES_TOTAL: integer (nullable = true)\n",
      " |-- NB_MORTS: integer (nullable = true)\n",
      " |-- NB_BLESSES_GRAVES: integer (nullable = true)\n",
      " |-- NB_BLESS_LEGERS: integer (nullable = true)\n",
      " |-- NB_DECES_PIETON: integer (nullable = true)\n",
      " |-- NB_BLESSES_PIETON: integer (nullable = true)\n",
      " |-- NB_VICTIMES_PIETON: integer (nullable = true)\n",
      " |-- NB_DECES_MOTO: integer (nullable = true)\n",
      " |-- NB_BLESSES_MOTO: integer (nullable = true)\n",
      " |-- NB_VICTIMES_MOTO: integer (nullable = true)\n",
      " |-- NB_DECES_VELO: integer (nullable = true)\n",
      " |-- NB_BLESSES_VELO: integer (nullable = true)\n",
      " |-- NB_VICTIMES_VELO: integer (nullable = true)\n",
      " |-- nb_automobile_camion_leger: integer (nullable = true)\n",
      " |-- nb_camionLourd_tractRoutier: integer (nullable = true)\n",
      " |-- nb_outil_equipement: integer (nullable = true)\n",
      " |-- nb_tous_autobus_minibus: integer (nullable = true)\n",
      " |-- nb_bicyclette: integer (nullable = true)\n",
      " |-- nb_cyclomoteur: integer (nullable = true)\n",
      " |-- nb_motocyclette: integer (nullable = true)\n",
      " |-- nb_taxi: integer (nullable = true)\n",
      " |-- nb_urgence: integer (nullable = true)\n",
      " |-- nb_motoneige: integer (nullable = true)\n",
      " |-- nb_VHR: integer (nullable = true)\n",
      " |-- nb_autres_types: integer (nullable = true)\n",
      " |-- nb_veh_non_precise: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  128647\n",
      "+-------------------+--------+-----+-----+\n",
      "|               DATE|WEEK_DAY|LIGHT|METEO|\n",
      "+-------------------+--------+-----+-----+\n",
      "|2012-02-01 00:00:00|      ME|    1|   11|\n",
      "|2012-06-28 00:00:00|      JE|    1|   11|\n",
      "|2012-07-11 00:00:00|      ME|    3|   11|\n",
      "|2012-01-03 00:00:00|      MA|    1|   11|\n",
      "|2012-01-05 00:00:00|      JE|    3|   11|\n",
      "|2012-01-05 00:00:00|      JE|    3|   17|\n",
      "|2012-01-07 00:00:00|      SA|    1|   11|\n",
      "|2012-01-09 00:00:00|      LU|    1|   12|\n",
      "|2012-01-11 00:00:00|      ME|    1|   12|\n",
      "|2012-01-12 00:00:00|      JE|    2|   18|\n",
      "+-------------------+--------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('DATE', 'WEEK_DAY', 'LIGHT', 'METEO')\n",
    "print('Number of rows: ', df.count())\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch timestamp to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DATE', df['DATE'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+-----+-----+\n",
      "|      DATE|WEEK_DAY|LIGHT|METEO|month|\n",
      "+----------+--------+-----+-----+-----+\n",
      "|2012-02-01|      ME|    1|   11|    2|\n",
      "|2012-06-28|      JE|    1|   11|    6|\n",
      "|2012-07-11|      ME|    3|   11|    7|\n",
      "|2012-01-03|      MA|    1|   11|    1|\n",
      "|2012-01-05|      JE|    3|   11|    1|\n",
      "|2012-01-05|      JE|    3|   17|    1|\n",
      "|2012-01-07|      SA|    1|   11|    1|\n",
      "|2012-01-09|      LU|    1|   12|    1|\n",
      "|2012-01-11|      ME|    1|   12|    1|\n",
      "|2012-01-12|      JE|    2|   18|    1|\n",
      "|2012-01-12|      JE|    3|   18|    1|\n",
      "|2012-01-13|      VE|    3|   12|    1|\n",
      "|2012-01-13|      VE|    1|   17|    1|\n",
      "|2012-01-14|      SA|    3|   12|    1|\n",
      "|2012-01-14|      SA|    1|   11|    1|\n",
      "|2012-01-15|      DI|    2|   11|    1|\n",
      "|2012-01-15|      DI|    2|   11|    1|\n",
      "|2012-01-18|      ME|    1|   11|    1|\n",
      "|2012-01-16|      LU|    1|   11|    1|\n",
      "|2012-01-18|      ME|    3|   11|    1|\n",
      "+----------+--------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('month', F.month(df.DATE))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the records where meteo is precised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128143\n",
      "128143\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "df = df.where(df.METEO != 99)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2012-01-01|   39|\n",
      "|2012-01-02|   24|\n",
      "|2012-01-03|   40|\n",
      "|2012-01-04|   43|\n",
      "|2012-01-05|   59|\n",
      "|2012-01-06|   40|\n",
      "|2012-01-07|   45|\n",
      "|2012-01-08|   35|\n",
      "|2012-01-09|   42|\n",
      "|2012-01-10|   47|\n",
      "|2012-01-11|   48|\n",
      "|2012-01-12|   78|\n",
      "|2012-01-13|  103|\n",
      "|2012-01-14|   95|\n",
      "|2012-01-15|   67|\n",
      "|2012-01-16|   76|\n",
      "|2012-01-17|  111|\n",
      "|2012-01-18|  109|\n",
      "|2012-01-19|   90|\n",
      "|2012-01-20|   65|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(['date']).count().sort('date', accending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing each day into two: day time and night time\n",
    "\n",
    "- Will first group codes 1 and 2 into DAY (code 1) and 3 and 4 into NIGHT (code 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      date|light|count|\n",
      "+----------+-----+-----+\n",
      "|2012-01-01|    1|    6|\n",
      "|2012-01-01|    2|    1|\n",
      "|2012-01-01|    3|   32|\n",
      "|2012-01-02|    3|   12|\n",
      "|2012-01-02|    1|   12|\n",
      "|2012-01-03|    2|    1|\n",
      "|2012-01-03|    3|   15|\n",
      "|2012-01-03|    1|   23|\n",
      "|2012-01-03|    4|    1|\n",
      "|2012-01-04|    3|   15|\n",
      "|2012-01-04|    1|   25|\n",
      "|2012-01-04|    2|    3|\n",
      "|2012-01-05|    2|    5|\n",
      "|2012-01-05|    1|   35|\n",
      "|2012-01-05|    4|    1|\n",
      "|2012-01-05|    3|   18|\n",
      "|2012-01-06|    2|    2|\n",
      "|2012-01-06|    3|   12|\n",
      "|2012-01-06|    1|   26|\n",
      "|2012-01-07|    3|   16|\n",
      "+----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(['date', 'light']).count().sort('date', accending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row:  DATE|WEEK_DAY|LIGHT|METEO|month\n",
    "#result: \"date\", \"week_day\", \"light\", \"meteo\", \"month\"]\n",
    "\n",
    "def assign_light(row):\n",
    "    result = [row[0], row[1], 0, row[3], row[4]]\n",
    "    day = [1, 2]\n",
    "    night = [3, 4]\n",
    "    if row[2] in day:\n",
    "        result[2] = 1\n",
    "        result[0] = str(result[0]) + '-d'\n",
    "    else:\n",
    "        result[2] = 2\n",
    "        result[0] = str(result[0]) + '-n'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-----+-----+\n",
      "|        date|week_day|light|meteo|month|\n",
      "+------------+--------+-----+-----+-----+\n",
      "|2012-02-01-d|      ME|    1|   11|    2|\n",
      "|2012-06-28-d|      JE|    1|   11|    6|\n",
      "|2012-07-11-n|      ME|    2|   11|    7|\n",
      "|2012-01-03-d|      MA|    1|   11|    1|\n",
      "|2012-01-05-n|      JE|    2|   11|    1|\n",
      "|2012-01-05-n|      JE|    2|   17|    1|\n",
      "|2012-01-07-d|      SA|    1|   11|    1|\n",
      "|2012-01-09-d|      LU|    1|   12|    1|\n",
      "|2012-01-11-d|      ME|    1|   12|    1|\n",
      "|2012-01-12-d|      JE|    1|   18|    1|\n",
      "|2012-01-12-n|      JE|    2|   18|    1|\n",
      "|2012-01-13-n|      VE|    2|   12|    1|\n",
      "|2012-01-13-d|      VE|    1|   17|    1|\n",
      "|2012-01-14-n|      SA|    2|   12|    1|\n",
      "|2012-01-14-d|      SA|    1|   11|    1|\n",
      "|2012-01-15-d|      DI|    1|   11|    1|\n",
      "|2012-01-15-d|      DI|    1|   11|    1|\n",
      "|2012-01-18-d|      ME|    1|   11|    1|\n",
      "|2012-01-16-d|      LU|    1|   11|    1|\n",
      "|2012-01-18-n|      ME|    2|   11|    1|\n",
      "+------------+--------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = df.rdd.map(assign_light).toDF([\"date\", \"week_day\", \"light\", \"meteo\", \"month\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-----+-----+\n",
      "|        date|week_day|light|meteo|month|\n",
      "+------------+--------+-----+-----+-----+\n",
      "|2012-01-01-d|      DI|    1|   12|    1|\n",
      "|2012-01-01-d|      DI|    1|   11|    1|\n",
      "|2012-01-01-d|      DI|    1|   12|    1|\n",
      "|2012-01-01-d|      DI|    1|   11|    1|\n",
      "|2012-01-01-d|      DI|    1|   12|    1|\n",
      "|2012-01-01-d|      DI|    1|   12|    1|\n",
      "|2012-01-01-d|      DI|    1|   13|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   17|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   14|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   17|    1|\n",
      "|2012-01-01-n|      DI|    2|   11|    1|\n",
      "|2012-01-01-n|      DI|    2|   12|    1|\n",
      "+------------+--------+-----+-----+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('date', accending=True).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming a prediction dataframe\n",
    "\n",
    "We will have folowing dataframes:\n",
    "\n",
    "- _dates_ - date and number of accidents\n",
    "- _df_pred_ - temporary dataframe that will be used in prediction\n",
    "- _df_p_ - final subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+\n",
      "|        date|light|count|\n",
      "+------------+-----+-----+\n",
      "|2012-01-01-d|    1|    7|\n",
      "|2012-01-01-n|    2|   32|\n",
      "|2012-01-02-d|    1|   12|\n",
      "|2012-01-02-n|    2|   12|\n",
      "|2012-01-03-d|    1|   24|\n",
      "|2012-01-03-n|    2|   16|\n",
      "|2012-01-04-d|    1|   28|\n",
      "|2012-01-04-n|    2|   15|\n",
      "|2012-01-05-d|    1|   40|\n",
      "|2012-01-05-n|    2|   19|\n",
      "+------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates = df.groupby('date', 'light').count().sort('date', accending=True)\n",
    "dates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list = list(dates.select('date').toPandas()['date'])              # create a list of unique dates\n",
    "c_list = list(dates.select('count').toPandas()['count'])            # create a list of the number of collision for each day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with METEO\n",
    "\n",
    "Adding Meteo columns:\n",
    "\n",
    "-  splitting each code into one column with number of accidents corresponding each code\n",
    "-  grouping them into fewer columns\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------+-----+-----+-----+\n",
      "|        date|light|week_day|month|meteo|count|\n",
      "+------------+-----+--------+-----+-----+-----+\n",
      "|2012-01-01-d|    1|      DI|    1|   13|    1|\n",
      "|2012-01-01-d|    1|      DI|    1|   12|    4|\n",
      "|2012-01-01-d|    1|      DI|    1|   11|    2|\n",
      "|2012-01-01-n|    2|      DI|    1|   17|    2|\n",
      "|2012-01-01-n|    2|      DI|    1|   14|    8|\n",
      "|2012-01-01-n|    2|      DI|    1|   12|   12|\n",
      "|2012-01-01-n|    2|      DI|    1|   11|   10|\n",
      "|2012-01-02-d|    1|      LU|    1|   12|    3|\n",
      "|2012-01-02-d|    1|      LU|    1|   11|    9|\n",
      "|2012-01-02-n|    2|      LU|    1|   14|    2|\n",
      "|2012-01-02-n|    2|      LU|    1|   12|    3|\n",
      "|2012-01-02-n|    2|      LU|    1|   11|    7|\n",
      "|2012-01-03-d|    1|      MA|    1|   12|    1|\n",
      "|2012-01-03-d|    1|      MA|    1|   17|    1|\n",
      "|2012-01-03-d|    1|      MA|    1|   11|   22|\n",
      "|2012-01-03-n|    2|      MA|    1|   11|   15|\n",
      "|2012-01-03-n|    2|      MA|    1|   12|    1|\n",
      "|2012-01-04-d|    1|      ME|    1|   12|    6|\n",
      "|2012-01-04-d|    1|      ME|    1|   17|    4|\n",
      "|2012-01-04-d|    1|      ME|    1|   11|   18|\n",
      "|2012-01-04-n|    2|      ME|    1|   12|    3|\n",
      "|2012-01-04-n|    2|      ME|    1|   11|    2|\n",
      "|2012-01-04-n|    2|      ME|    1|   17|   10|\n",
      "|2012-01-05-d|    1|      JE|    1|   12|    8|\n",
      "|2012-01-05-d|    1|      JE|    1|   11|   30|\n",
      "|2012-01-05-d|    1|      JE|    1|   17|    2|\n",
      "|2012-01-05-n|    2|      JE|    1|   17|    5|\n",
      "|2012-01-05-n|    2|      JE|    1|   12|    4|\n",
      "|2012-01-05-n|    2|      JE|    1|   11|   10|\n",
      "|2012-01-06-d|    1|      VE|    1|   11|    8|\n",
      "|2012-01-06-d|    1|      VE|    1|   13|    1|\n",
      "|2012-01-06-d|    1|      VE|    1|   17|    9|\n",
      "|2012-01-06-d|    1|      VE|    1|   12|   10|\n",
      "|2012-01-06-n|    2|      VE|    1|   12|    8|\n",
      "|2012-01-06-n|    2|      VE|    1|   11|    3|\n",
      "|2012-01-06-n|    2|      VE|    1|   17|    1|\n",
      "|2012-01-07-d|    1|      SA|    1|   12|   11|\n",
      "|2012-01-07-d|    1|      SA|    1|   19|    3|\n",
      "|2012-01-07-d|    1|      SA|    1|   11|    6|\n",
      "|2012-01-07-d|    1|      SA|    1|   17|    5|\n",
      "|2012-01-07-d|    1|      SA|    1|   14|    4|\n",
      "|2012-01-07-n|    2|      SA|    1|   14|    4|\n",
      "|2012-01-07-n|    2|      SA|    1|   13|    1|\n",
      "|2012-01-07-n|    2|      SA|    1|   17|    1|\n",
      "|2012-01-07-n|    2|      SA|    1|   11|    6|\n",
      "|2012-01-07-n|    2|      SA|    1|   12|    4|\n",
      "|2012-01-08-d|    1|      DI|    1|   11|   16|\n",
      "|2012-01-08-d|    1|      DI|    1|   12|    4|\n",
      "|2012-01-08-n|    2|      DI|    1|   11|    9|\n",
      "|2012-01-08-n|    2|      DI|    1|   12|    5|\n",
      "+------------+-----+--------+-----+-----+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meteo = df.select('date', 'light', 'week_day', 'month', 'meteo').groupby('date', 'light', 'week_day', 'month', 'meteo').count().sort('date', accending=True)  \n",
    "df_meteo.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing dates list into 3 parts in order to process faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|        date|count|day|night| DI| LU| MA| ME| JE| VE| SA|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12|m-11|m-12|m-13|m-14|m-15|m-16|m-17|m-18|m-19|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|2012-01-01-d|    0|  0|    0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def assign_light(row):\n",
    "    # row structure: ['date', 'light', 'count', 'meteo', 'week_day', 'month']\n",
    "    # result structure: [\"day\", \"night\", 'DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA', \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"m_11\",\"m_12\", \"m_13\", \"m_14\", \"m_15\", \"m_16\", \"m_17\", \"m_18\", \"m_19\"]\n",
    "    result = [0 for i in range(30)]\n",
    "    \n",
    "    # meteo:\n",
    "    qty = row[2]                                                    #number of accidents with this meteo code (column count)\n",
    "    labels = [11,12,13,14,15,16,17,18,19]\n",
    "    for k in range(len(labels)):\n",
    "        if row[3] == labels[k]:\n",
    "            result[k+21] = qty\n",
    "    \n",
    "    # light:\n",
    "    day = 1\n",
    "    night = 2\n",
    "    if row[1] == day:\n",
    "        result[0] = 1\n",
    "    else:\n",
    "        result[1] = 1\n",
    "        \n",
    "    # week day\n",
    "    labels = ['DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA']\n",
    "    for k in range(len(labels)):\n",
    "        if row[4] == labels[k]:\n",
    "            result[k+2] = 1\n",
    "            \n",
    "    # month\n",
    "    labels = [i for i in range(1,13)]\n",
    "    for k in range(len(labels)):\n",
    "        if row[5] == labels[k]:\n",
    "            result[k+9] = 1\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "columns = ['date', 'count', \"day\", \"night\", 'DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA', \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"m-11\",\"m-12\", \"m-13\",\"m-14\", \"m-15\", \"m-16\", \"m-17\", \"m-18\", \"m-19\"]   # Setting uo the columns for prediction dataframe\n",
    "values = [d_list[0]]                                                # Creating temporary row with the first element as a date and the rest as an int\n",
    "for i in range(len(columns)-1):\n",
    "    values.append(0)\n",
    "df_pred = spark.createDataFrame([values],columns)                   # Creating dataframe\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the data in 4 batches: running cycle with differents __i__ values. Each part requires restarting kernel. Results are written into temp files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|        date|count|day|night| DI| LU| MA| ME| JE| VE| SA|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12|m-11|m-12|m-13|m-14|m-15|m-16|m-17|m-18|m-19|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|2014-01-20-d|   37|  1|    0|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  21|  14|   0|   0|   0|   0|   1|   1|   0|\n",
      "|2014-01-20-n|   12|  0|    1|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   6|   3|   0|   0|   0|   0|   3|   0|   0|\n",
      "|2014-01-21-d|   54|  1|    0|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  54|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-21-n|   14|  0|    1|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|   4|   0|   0|   0|   0|   1|   0|   0|\n",
      "|2014-01-22-d|   34|  1|    0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  34|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-22-n|   15|  0|    1|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  15|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-23-d|   39|  1|    0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  37|   2|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-23-n|   14|  0|    1|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  11|   2|   0|   0|   0|   0|   0|   1|   0|\n",
      "|2014-01-24-d|   49|  1|    0|  0|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  45|   4|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-24-n|   20|  0|    1|  0|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|  10|   0|   0|   0|   1|   0|   0|   0|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 1min 39s, sys: 53.2 s, total: 2min 33s\n",
      "Wall time: 1h 20min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# part-1\n",
    "    \n",
    "# for i in range(0, 1500):                                 # go through the array of dates\n",
    "for i in range(1500, 3000): \n",
    "# for i in range(3500, 4000): \n",
    "# for i in range(4500, len(d_list)): \n",
    "    final_list = [d_list[i], c_list[i]]                           # Forming the row starting by date and number of accidents\n",
    "    d = df_meteo.select('date', 'light', 'count', 'meteo', 'week_day', 'month').filter(df_meteo.date == d_list[i])  # getting accidents for specific date\n",
    "    dd = d.rdd.map(assign_light).toDF([\"day\", \"night\", 'DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA', \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"m_11\",\"m_12\", \"m_13\", \"m_14\", \"m_15\", \"m_16\", \"m_17\", \"m_18\", \"m_19\"])       # dataframe with METEO codes as columns and number of accidents as values\n",
    "    dd = dd.toPandas()\n",
    "\n",
    "    for i in range(21):\n",
    "        if type(dd.iloc[0,i]) == 'string':\n",
    "            final_list.append(dd.iloc[0,i])\n",
    "        else:\n",
    "            final_list.append(int(dd.iloc[0,i]))\n",
    "\n",
    "    for c in range(21, len(dd.columns)):                         # Reducing dataframe by calculating sum of accidents dor each METEO code for one day\n",
    "        s = int(dd.iloc[:,c].sum())\n",
    "        final_list.append(s)\n",
    "    newRow = spark.createDataFrame([final_list], columns)\n",
    "    df_pred = df_pred.union(newRow)                   # Adding a row into prediction dataset\n",
    "\n",
    "df_pred = df_pred.filter(df_pred['count'] != 0) # Removing temporary row\n",
    "df_pred.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing temp file. Don't forget to choose the line corresponding to the part number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/pred_1.csv', header=\"true\")\n",
    "# df_pred.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/pred_2.csv', header=\"true\")\n",
    "# df_pred.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/pred_3.csv', header=\"true\")\n",
    "# df_pred.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/pred_4.csv', header=\"true\")\n",
    "\n",
    "# df_pred.repartition(1).write.csv(path='/data/pred_1.csv', header=\"true\")\n",
    "df_pred.repartition(1).write.csv(path='/data/pred_2.csv', header=\"true\")\n",
    "# df_pred.repartition(1).write.csv(path='/data/pred_3.csv', header=\"true\")\n",
    "# df_pred.repartition(1).write.csv(path='/pred_4.csv', header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating meteo-features: grouping codes into 4 groups and assigning 1 to the one that gets the most amount of collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'count', 'day', 'night', 'DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', 'm-11', 'm-12', 'm-13', 'm-14', 'm-15', 'm-16', 'm-17', 'm-18', 'm-19']\n",
      "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def meteo(row):\n",
    "    # row structure: ['date', 'count', 'day', 'night', 'DI', 'LU', 'MA', 'ME', 'JE', 'VE', 'SA', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', 'm-11', 'm-12', 'm-13', 'm-14', 'm-15', 'm-16', 'm-17', 'm-18', 'm-19']\n",
    "#     result = [row[i] for i in range(23)]\n",
    "    \n",
    "    def meteo(a,b,c):\n",
    "        n = 0\n",
    "        if a != 0: \n",
    "            n = n + 1\n",
    "        if b != 0:\n",
    "            n = n + 1\n",
    "        if c != 0:\n",
    "            n = n + 1\n",
    "\n",
    "        if n != 0:\n",
    "            return((a + b + c) // n)\n",
    "        else:\n",
    "            return(a + b + c)\n",
    "\n",
    "    meteo_list = [0 for i in range(4)]\n",
    "    meteo_list[0] = meteo(row[0], row[1], 0)        # m_11 and m_12 to one group with average qty in not 0 (normal)\n",
    "    meteo_list[1] = meteo(row[2], row[3], row[4])  # m_13, m_14 and m_15 (rain)\n",
    "    meteo_list[2] = meteo(row[5], row[6], row[7])  # m_16, m_17 and m_18 (snow)\n",
    "    meteo_list[3] = int(row[8])                           # m_19 not grouped (ice)\n",
    "    m = pd.Series(meteo_list).max()\n",
    "    \n",
    "    result = [0 if i != m else 1 for i in meteo_list]\n",
    "    \n",
    "#     for i in meteo_list:\n",
    "#         result.append(i)\n",
    "    return result\n",
    "\n",
    "print(df_pred.columns)\n",
    "ddd = df_pred.select('m-11', 'm-12', 'm-13', 'm-14', 'm-15', 'm-16', 'm-17', 'm-18', 'm-19')\n",
    "df_add = ddd.rdd.map(meteo).toDF(['11-12', '13-14-15', '16-17-18', '19'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|        date|count|day|night| DI| LU| MA| ME| JE| VE| SA|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12|m-11|m-12|m-13|m-14|m-15|m-16|m-17|m-18|m-19|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "|2014-01-20-d|   37|  1|    0|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  21|  14|   0|   0|   0|   0|   1|   1|   0|\n",
      "|2014-01-20-n|   12|  0|    1|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   6|   3|   0|   0|   0|   0|   3|   0|   0|\n",
      "|2014-01-21-d|   54|  1|    0|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  54|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-21-n|   14|  0|    1|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|   4|   0|   0|   0|   0|   1|   0|   0|\n",
      "|2014-01-22-d|   34|  1|    0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  34|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-22-n|   15|  0|    1|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  15|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-23-d|   39|  1|    0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  37|   2|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-23-n|   14|  0|    1|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  11|   2|   0|   0|   0|   0|   0|   1|   0|\n",
      "|2014-01-24-d|   49|  1|    0|  0|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  45|   4|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-24-n|   20|  0|    1|  0|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|  10|   0|   0|   0|   1|   0|   0|   0|\n",
      "|2014-01-25-d|   35|  1|    0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|   7|   0|   0|   0|   1|  18|   0|   0|\n",
      "|2014-01-25-n|   15|  0|    1|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   2|   7|   0|   0|   0|   0|   5|   1|   0|\n",
      "|2014-01-26-d|   28|  1|    0|  1|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  22|   5|   0|   0|   0|   0|   1|   0|   0|\n",
      "|2014-01-26-n|   21|  0|    1|  1|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   8|   5|   0|   0|   0|   1|   6|   1|   0|\n",
      "|2014-01-27-d|   90|  1|    0|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  18|   1|   2|   0|   0|   0|  44|  25|   0|\n",
      "|2014-01-27-n|   23|  0|    1|  0|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   9|   5|   0|   0|   0|   1|   6|   2|   0|\n",
      "|2014-01-28-d|   40|  1|    0|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  35|   3|   0|   0|   0|   0|   1|   1|   0|\n",
      "|2014-01-28-n|   16|  0|    1|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  12|   4|   0|   0|   0|   0|   0|   0|   0|\n",
      "|2014-01-29-d|   40|  1|    0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  31|   8|   0|   0|   0|   0|   1|   0|   0|\n",
      "|2014-01-29-n|   14|  0|    1|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   8|   5|   0|   0|   0|   0|   1|   0|   0|\n",
      "+------------+-----+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+---+\n",
      "|11-12|13-14-15|16-17-18| 19|\n",
      "+-----+--------+--------+---+\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    0|       0|       1|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    0|       0|       1|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "|    1|       0|       0|  0|\n",
      "+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_add.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding indexes to both of the temp datasets to be able to merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df_pred = df_pred.select(\"*\").withColumn(\"index\", monotonically_increasing_id())\n",
    "df_add = df_add.select(\"*\").withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_pred.join(df_add, df_pred.index == df_add.index)\n",
    "df_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.drop('m-11', 'm-12', 'm-13', 'm-14', 'm-15', 'm-16', 'm-17', 'm-18', 'm-19')\n",
    "df_p = df_p.drop(df_add['index'])\n",
    "df_p = df_p.drop(df_pred['index']).sort('date', acsending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the final file. Don't forget to choose the line corresponding to the part number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_p.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/final_part_1.csv', header=\"true\")\n",
    "# df_p.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/final_part_2.csv', header=\"true\")\n",
    "# df_p.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/final_part_3.csv', header=\"true\")\n",
    "# df_p.repartition(1).write.csv(path='gs://jupyter-empty-global/notebooks/jupyter/data/final_part_4.csv', header=\"true\")\n",
    "\n",
    "# df_p.repartition(1).write.csv(path='/data/final_part_1.csv', header=\"true\")\n",
    "df_p.repartition(1).write.csv(path='/data/final_part_2.csv', header=\"true\")\n",
    "# df_p.repartition(1).write.csv(path='/data/final_part_3.csv', header=\"true\")\n",
    "# df_p.repartition(1).write.csv(path='/data/final_part_4.csv', header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all the parts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_pred_1 = spark.read.csv('data/final_part_1.csv', header='true', inferSchema = True)\n",
    "# df_pred_2 = spark.read.csv('data/final_part_2.csv', header='true', inferSchema = True)\n",
    "# df_pred_3 = spark.read.csv('data/final_part_3.csv', header='true', inferSchema = True)\n",
    "# df_pred_4 = spark.read.csv('data/final_part_4.csv', header='true', inferSchema = True)\n",
    "\n",
    "# from functools import reduce  # For Python 3.x\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def unionAll(*dfs):\n",
    "#     return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "# df_all = unionAll(df_pred_1, df_pred_2, df_pred_3, df_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = df_pred_1.union(df_pred_2).union(df_pred_3).union(df_pred_4)\n",
    "# df_all.count()\n",
    "# df_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.repartition(1).write.csv(path='/data/final_all.csv', header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
